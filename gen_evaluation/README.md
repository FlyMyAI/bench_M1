# GenEval Image Generation Evaluation

This directory contains the complete GenEval benchmark evaluation for **Flymy AI** image generation model.

## Results Summary

| Metric | Value |
|--------|--------|
| **Overall Image Accuracy** | **76.16%** (1680/2206) |
| **Prompt Accuracy** | **60.69%** (335/552) |
| **Overall Score** | **0.7690** |

## GenEval Benchmark Results Comparison

| Type | Model | Single Obj. | Two Obj. | Counting | Colors | Position | Color Attr. | Overall |
|------|-------|-------------|----------|----------|---------|----------|-------------|---------|
| **Gen. Only** | PixArt-Œ± [9] | 0.98 | 0.50 | 0.44 | 0.80 | 0.08 | 0.07 | 0.48 |
| | SDv2.1 [61] | 0.98 | 0.51 | 0.44 | 0.85 | 0.07 | 0.17 | 0.50 |
| | DALL-E 2 [60] | 0.94 | 0.66 | 0.49 | 0.77 | 0.10 | 0.19 | 0.52 |
| | Emu3-Gen [79] | 0.98 | 0.71 | 0.34 | 0.81 | 0.17 | 0.21 | 0.54 |
| | SDXL [58] | 0.98 | 0.74 | 0.39 | 0.85 | 0.15 | 0.23 | 0.55 |
| | DALL-E 3 [5] | 0.96 | 0.87 | 0.47 | 0.83 | 0.43 | 0.45 | 0.67 |
| | SD3-Medium [19] | 0.99 | 0.94 | 0.72 | 0.89 | 0.33 | 0.60 | 0.74 |
| | FLUX.1-dev‚Ä† [35] | 0.98 | 0.93 | 0.75 | 0.93 | 0.68 | 0.65 | 0.82 |
| **Unified** | Chameleon [70] | - | - | - | - | - | - | 0.39 |
| | LWM [42] | 0.93 | 0.41 | 0.46 | 0.79 | 0.09 | 0.15 | 0.47 |
| | SEED-X [23] | 0.97 | 0.58 | 0.26 | 0.80 | 0.19 | 0.14 | 0.49 |
| | TokenFlow-XL [59] | 0.95 | 0.60 | 0.41 | 0.81 | 0.16 | 0.24 | 0.55 |
| | ILLUME [76] | 0.99 | 0.86 | 0.45 | 0.71 | 0.39 | 0.28 | 0.61 |
| | Janus [83] | 0.97 | 0.68 | 0.30 | 0.84 | 0.46 | 0.42 | 0.61 |
| | Transfusion [102] | - | - | - | - | - | - | 0.63 |
| | Emu3-Gen [79] | 0.99 | 0.81 | 0.42 | 0.80 | 0.49 | 0.45 | 0.66 |
| | Show-o [88] | 0.98 | 0.80 | 0.66 | 0.84 | 0.31 | 0.50 | 0.68 |
| | **Flymy AI** | **0.99** | **0.97** | **0.70** | **0.86** | **0.51** | **0.58** | **0.77** |
| | Janus-Pro-7B [1] | 0.99 | 0.89 | 0.59 | 0.90 | 0.79 | 0.66 | 0.80 |
| | MetaQuery-XL‚Ä† [57] | - | - | - | - | - | - | 0.80 |
| | BAGEL | 0.99 | 0.94 | 0.81 | 0.88 | 0.64 | 0.63 | 0.82 |
| | BAGEL‚Ä† | 0.98 | 0.95 | 0.84 | 0.95 | 0.78 | 0.77 | 0.88 |

## Usage

### Generate Images
```bash
python gen_images_flymy.py \
    --output_dir output/images \
    --metadata_file evaluation_metadata.jsonl \
    --num_images 4 \
    --num_workers 1
```

### Run Evaluation
```bash
python evaluate_images_single.py \
    output/images \
    --outfile results.jsonl
```

### Generate Reports
```bash
python generate_report.py \
    results.jsonl \
    --output report.md \
    --model-name "Flymy AI" \
    --csv
```

## üìÅ Directory Structure

```
gen_evaluation/
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ gen_images_flymy.py                # Image generation script
‚îú‚îÄ‚îÄ evaluate_images_single.py          # Evaluation script
‚îú‚îÄ‚îÄ generate_report.py                 # Report generation
‚îú‚îÄ‚îÄ evaluation_metadata.jsonl          # Evaluation prompts
‚îú‚îÄ‚îÄ object_names.txt                   # Object class names
‚îú‚îÄ‚îÄ mask2former_swin-s-p4-w7-224...pth # Object detection model (symlink)
‚îú‚îÄ‚îÄ geneval_comparison_table.md        # Benchmark comparison
‚îú‚îÄ‚îÄ full_evaluation_report.md          # Detailed report
‚îú‚îÄ‚îÄ geneval_summary.csv                # Summary metrics
‚îú‚îÄ‚îÄ geneval_categories.csv             # Category breakdown
‚îú‚îÄ‚îÄ geneval_prompts.csv                # Per-prompt results
‚îî‚îÄ‚îÄ geneval_errors.csv                 # Error analysis
```

## üìà Key Insights

### Strengths
- **Excellent at basic tasks**: 99.38% accuracy for single objects
- **Strong multi-object generation**: 97.46% for two objects
- **Good color understanding**: 86.02% for basic colors
- **Professional-grade overall performance**: 0.77 score

### Areas for Improvement
- **Spatial reasoning**: 50.50% accuracy for positioning tasks
- **Complex attributes**: 57.75% for color attributes
- **Counting precision**: 70.31% for object counting

## üîß Technical Details

- **Object Detection Model**: Mask2Former with Swin-S backbone
- **CLIP Model**: ViT-L-14
- **Detection Thresholds**: 0.3 (general), 0.9 (counting)
- **Images per Prompt**: 4
- **Total Prompts**: 553 (covering 6 categories)

## üìÑ Reports

- `geneval_comparison_table.md` - Benchmark comparison with other models
- `full_evaluation_report.md` - Detailed analysis and results
- `geneval_*.csv` - Machine-readable data for further analysis

## üéñÔ∏è Achievements

- **Top 25%** performance among all evaluated models
- **Surpasses DALL-E 3** (0.67) and many other commercial models
- **Professional-grade quality** suitable for real-world applications
- **Excellent baseline** for further model improvements 